{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consistent-scanner",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM, Embedding, Dropout, SimpleRNN, Bidirectional\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/labeled/combined.csv\")\n",
    "electronics = df.groupby(df.category).get_group(\"Electronics\")\n",
    "pet = df.groupby(df.category).get_group(\"Pet supplies\")\n",
    "baby = df.groupby(df.category).get_group(\"Baby\")\n",
    "sports = df.groupby(df.category).get_group(\"Sport outdoors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-formula",
   "metadata": {},
   "source": [
    "## Create a balanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-constraint",
   "metadata": {},
   "source": [
    "### Truncate Sentences to 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_sentence(sentence, max_words=50):\n",
    "    return ' '.join(sentence.split()[:max_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-observer",
   "metadata": {},
   "source": [
    "### Split Dataframe to test & training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-russia",
   "metadata": {},
   "source": [
    "## Initialize BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "    \n",
    "def get_BERT_subwords(text):\n",
    "    input_ids = bert_tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "    subwords = bert_tokenizer.tokenize(text)\n",
    "    return subwords\n",
    "    \n",
    "def create_BERT_vectors(text):\n",
    "    #input_ids = tokenizer(text, max_length=50, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = bert_tokenizer(text, max_length=100, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    output = bert_model(**input_ids)\n",
    "    \n",
    "    final_layer = output.last_hidden_state\n",
    "    return final_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-orbit",
   "metadata": {},
   "source": [
    "### Check max length of tokens in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_max_length():\n",
    "    max_token_len = 0\n",
    "    longest_text = \"\"\n",
    "    for sentence in df[\"sentence\"].values:\n",
    "        tokens = get_BERT_subwords(sentence)\n",
    "        token_len = len(tokens)\n",
    "        if token_len > max_token_len:\n",
    "            max_token_len = token_len\n",
    "            longest_text  = sentence\n",
    "    print(max_token_len)\n",
    "    print(longest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-coalition",
   "metadata": {},
   "source": [
    "# Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-radiation",
   "metadata": {},
   "source": [
    "## Initialize XLNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "xlnet_model = XLNetModel.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "def create_XLNet_vectors(text):\n",
    "    inputs = xlnet_tokenizer(text, max_length=100, padding='max_length', return_tensors=\"pt\")\n",
    "    outputs = xlnet_model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    return last_hidden_states\n",
    "\n",
    "def get_XLNet_subwords(text):\n",
    "    input_ids = xlnet_tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "    subwords = xlnet_tokenizer.tokenize(text)\n",
    "    return subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-shift",
   "metadata": {},
   "source": [
    "## Initialize Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=300)   # embedding size = 100\n",
    "\n",
    "def create_GLOVE_vector(text):\n",
    "    text_glove = text.split()\n",
    "    vector = glove[text_glove[0]]\n",
    "    final_layer = vector[None, :]\n",
    "   \n",
    "    \n",
    "    for i in range(len(text_glove)-1):\n",
    "        vector = glove[text_glove[i+1]]\n",
    "        vector = vector[None, :]\n",
    "        final_layer = tf.concat([final_layer, vector], axis=0)\n",
    "\n",
    "    if final_layer.shape[0] != 50:\n",
    "        difference = 50 - final_layer.shape[0]\n",
    "        final_layer= np.pad(final_layer, ((0, difference),(0,0)))\n",
    "        final_layer = final_layer[None,:,:]\n",
    "    else:\n",
    "        final_layer = tf.expand_dims(final_layer, axis=0)\n",
    "    return final_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-hacker",
   "metadata": {},
   "source": [
    "## Concatenate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-stand",
   "metadata": {},
   "source": [
    "### Concatenate BERT Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "word_count = 1\n",
    "next_vector = False\n",
    "\n",
    "def conc_BERT_subwords(text):\n",
    "    vector = create_BERT_vectors(text)\n",
    "    subwords = get_BERT_subwords(text)\n",
    "    sum_vector = torch.empty((1,768))\n",
    "    \n",
    "    i = 0\n",
    "    word_count = 1\n",
    "    max_length = 50 ## Ab wo soll gecutted werden? Notwendig durch verschiedene Subwords von XLN/BERT\n",
    "    \n",
    "    next_vector = False\n",
    "    #print(vector.size())\n",
    "    #print(subwords)\n",
    "    while i < len(subwords):\n",
    "        #print(\"Word: \", subwords[i])\n",
    "        #print(\"Index: \", i)\n",
    "\n",
    "        ## Prüfe ob dieses Wort mit # beginnt\n",
    "        if (subwords[i].startswith('#') == True):\n",
    "            word_count = word_count + 1\n",
    "\n",
    "            #Prüfe ob es das letzt Wort im Array ist\n",
    "            if (i != len(subwords)-1):\n",
    "                ## Prüfe ob nächstes Wort mit # beginnt\n",
    "                if (subwords[i+1].startswith('#')== True):\n",
    "                    next_vector = True\n",
    "                else:\n",
    "                    next_vector = False\n",
    "            else:\n",
    "                ## Wenn es das letzt Wort ist, kann das nächste Wort nicht mit # anfangen\n",
    "                next_vector = False            \n",
    "\n",
    "            if next_vector == True:            \n",
    "                ## Prüfe ob nächstes Wort das erste der Subwords ist\n",
    "                if (word_count == 2):\n",
    "                    sum_vector = vector[0][i] + vector[0][i+1]\n",
    "                else:\n",
    "                    sum_vector = sum_vector + vector[0][i+1]\n",
    "\n",
    "            ## Wenn das nächste Wort nicht mit # beginnt        \n",
    "            else:\n",
    "                if (word_count == 2):\n",
    "                    sum_vector = vector[0][i] + vector[0][i+1]\n",
    "                else:\n",
    "                    sum_vector = sum_vector + vector[0][i+1]\n",
    "                sum_vector = sum_vector / word_count\n",
    "                vector[0][i] = sum_vector\n",
    "                #print(\"Vektor [0][\",i,\"] ersetzt\")\n",
    "                word_count = 1\n",
    "\n",
    "            ## Entferne Vektor der gerade dazugerechnet wurde\n",
    "            vector = torch.cat((vector[:,:i+1],vector[:,i+2:]),1) \n",
    "            #print(subwords[i], \" deleted\")\n",
    "            #print(\"Vector [0][\", i+1 ,\"] deleted\")\n",
    "            del subwords[i]\n",
    "            i = i-1\n",
    "            \n",
    "        i = i+1\n",
    "        \n",
    "    #print(subwords)\n",
    "    vector = vector[:,:max_length]\n",
    "    #print(vector.size())\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-bobby",
   "metadata": {},
   "source": [
    "### Concatenate XLNet Subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "word_count = 1\n",
    "next_vector = False\n",
    "\n",
    "def conc_XLNet_subwords(text):\n",
    "    vector = create_XLNet_vectors(text)\n",
    "    subwords = get_XLNet_subwords(text)\n",
    "    sum_vector = torch.empty((1,768))\n",
    "\n",
    "    i = 0\n",
    "    word_count = 1\n",
    "    next_vector = False\n",
    "    max_length = 50 ## Ab wo soll gecutted werden? Notwendig durch verschiedene Subwords von XLN/BERT\n",
    "\n",
    "    #print(vector.size())\n",
    "    #print(subwords)\n",
    "    while i < len(subwords):\n",
    "        #print(\"Word: \", subwords[i])\n",
    "        #print(\"Index: \", i)\n",
    "\n",
    "        ## Prüfe ob dieses Wort nicht mit ▁ beginnt\n",
    "        if (subwords[i].startswith('▁') == False):\n",
    "            word_count = word_count + 1\n",
    "\n",
    "            #Prüfe ob es das letzt Wort im Array ist\n",
    "            if (i != len(subwords)-1):\n",
    "                ## Prüfe ob nächstes Wort mit ▁ beginnt\n",
    "                if (subwords[i+1].startswith('▁')== False):\n",
    "                    next_vector = True\n",
    "                else:\n",
    "                    next_vector = False\n",
    "            else:\n",
    "                ## Wenn es das letzt Wort ist, kann das nächste Wort nicht mit ▁ anfangen\n",
    "                next_vector = False            \n",
    "\n",
    "            if next_vector == True:            \n",
    "                ## Prüfe ob nächstes Wort das erste der Subwords ist\n",
    "                if (word_count == 2):\n",
    "                    sum_vector = vector[0][i] + vector[0][i+1]\n",
    "                else:\n",
    "                    sum_vector = sum_vector + vector[0][i+1]\n",
    "\n",
    "            ## Wenn das nächste Wort nicht mit ▁ beginnt        \n",
    "            else:\n",
    "                if (word_count == 2):\n",
    "                    sum_vector = vector[0][i] + vector[0][i+1]\n",
    "                else:\n",
    "                    sum_vector = sum_vector + vector[0][i+1]\n",
    "                sum_vector = sum_vector / word_count\n",
    "                vector[0][i] = sum_vector\n",
    "                #print(\"Vektor [0][\",i,\"] ersetzt\")\n",
    "                word_count = 1\n",
    "\n",
    "            ## Entferne Vektor der gerade dazugerechnet wurde\n",
    "            vector = torch.cat((vector[:,:i+1],vector[:,i+2:]),1) \n",
    "            #print(subwords[i], \" deleted\")\n",
    "            #print(\"Vector [0][\", i+1 ,\"] deleted\")\n",
    "            del subwords[i]\n",
    "            i = i-1\n",
    "            \n",
    "        i = i+1\n",
    "    #print(subwords)\n",
    "    vector = vector[:,:max_length]\n",
    "    #print(vector.size())\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        # y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1 - p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.mean(K.sum(loss, axis=1))\n",
    "        return loss\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-budget",
   "metadata": {},
   "source": [
    "df_s_train = df.head(10)\n",
    "df_s_test = df.head(10)\n",
    "\n",
    "df_s_train['vector'] = df_s_test.apply(lambda x: generate_conc_vector(x['sentence']), axis=1)\n",
    "df_s_test['vector'] = df_s_test.apply(lambda x: generate_conc_vector(x['sentence']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(train_df, test_df):\n",
    "    train_df['sentence'] = train_df.sentence.apply(truncate_sentence)\n",
    "    test_df['sentence'] = test_df.sentence.apply(truncate_sentence)\n",
    "    \n",
    "    x_train = np.asarray(train_df.sentence)\n",
    "    x_test = np.asarray(test_df.sentence)\n",
    "    y_train = np.asarray(train_df.label)\n",
    "    y_test = np.asarray(test_df.label)\n",
    "            \n",
    "    # bert\n",
    "    print(\"-- starting bert --\")\n",
    "    bert_vectors = conc_BERT_subwords(x_train[0]).detach().numpy()\n",
    "    for i in tqdm(range(len(x_train)-1)):\n",
    "        vector = conc_BERT_subwords(x_train[i+1]).detach().numpy()\n",
    "        bert_vectors = tf.concat([bert_vectors, vector], axis=0)\n",
    "    \n",
    "    bert_vectors_test = conc_BERT_subwords(x_test[0]).detach().numpy()\n",
    "    for i in tqdm(range(len(x_test)-1)):\n",
    "        vector = conc_BERT_subwords(x_test[i+1]).detach().numpy()\n",
    "        bert_vectors_test = tf.concat([bert_vectors_test, vector], axis=0)\n",
    "        \n",
    "    print(\"-- starting Xl-NET --\")\n",
    "    XLNet_vectors = conc_XLNet_subwords(x_train[0]).detach().numpy()\n",
    "    for i in tqdm(range(len(x_train)-1)):\n",
    "        vector = conc_XLNet_subwords(x_train[i+1]).detach().numpy()\n",
    "        XLNet_vectors = tf.concat([XLNet_vectors, vector], axis=0)\n",
    "\n",
    "    XLNet_vectors_test = conc_XLNet_subwords(x_test[0]).detach().numpy()\n",
    "    for i in tqdm(range(len(x_test)-1)):\n",
    "        vector = conc_XLNet_subwords(x_test[i+1]).detach().numpy()\n",
    "        XLNet_vectors_test = tf.concat([XLNet_vectors_test, vector], axis=0)\n",
    "        \n",
    "   \n",
    "        \n",
    "     # glove\n",
    "    print(\"-- starting glove --\")   \n",
    "    glove_vectors = []\n",
    "    for i in tqdm(range(len(x_train))):\n",
    "        vector = create_GLOVE_vector(x_train[i])\n",
    "        if len(glove_vectors) == 0:\n",
    "            glove_vectors = vector\n",
    "        else:\n",
    "            glove_vectors = tf.concat([glove_vectors, vector], axis=0)\n",
    "    \n",
    "    glove_vectors_test = []\n",
    "    for i in tqdm(range(len(x_test))):\n",
    "        vector = create_GLOVE_vector(x_test[i])\n",
    "        if len(glove_vectors_test) == 0:\n",
    "            glove_vectors_test = vector\n",
    "        else:\n",
    "            glove_vectors_test = tf.concat([glove_vectors_test, vector], axis=0)\n",
    "        \n",
    "    print(\"-- concat vectors --\")\n",
    "    conc_vector = tf.concat([glove_vectors, bert_vectors, XLNet_vectors], axis=2)\n",
    "    print(type(conc_vector))\n",
    "    print(conc_vector.shape)\n",
    "    print(type(y_train))\n",
    "\n",
    "    conc_vector_test = tf.concat([glove_vectors_test, bert_vectors_test, XLNet_vectors_test], axis=2)\n",
    "    conc_vector_test.shape\n",
    "    \n",
    "    print(\"-- create model --\")\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(50,2348) ,name='Conc-Vector'))\n",
    "    model.add(Dropout(0.1, input_shape=(50,2348)))\n",
    "    model.add(SimpleRNN(512, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(256)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #model.summary()\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=[binary_focal_loss(alpha=.8, gamma=2)],\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    model.fit(conc_vector, y_train, epochs=7, validation_data=(conc_vector_test, y_test))\n",
    "    y_preds = model.predict(conc_vector_test)\n",
    "    y_preds = np.round(y_preds)\n",
    "    \n",
    "    f1 = f1_score(y_test,y_preds, average=\"macro\")\n",
    "    acc = accuracy_score(y_test, y_preds)\n",
    "    mcc = matthews_corrcoef(y_test,y_preds)\n",
    "    return f1, acc, mcc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-pulse",
   "metadata": {},
   "source": [
    "### load data and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/labeled/combined.csv\")\n",
    "electronics = df.groupby(df.category).get_group(\"Electronics\")\n",
    "pet = df.groupby(df.category).get_group(\"Pet supplies\")\n",
    "baby = df.groupby(df.category).get_group(\"Baby\")\n",
    "sports = df.groupby(df.category).get_group(\"Sport outdoors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 2)\n",
    "data = []\n",
    "\n",
    "for train_index , test_index in kf.split(baby):\n",
    "    data_df = baby\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"baby\",f1,acc, mcc])\n",
    "    \n",
    "for train_index , test_index in kf.split(pet):\n",
    "    data_df = pet\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"pet\",f1,acc, mcc])\n",
    "\n",
    "for train_index , test_index in kf.split(sports):\n",
    "    data_df = sports\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"sports\",f1,acc, mcc])\n",
    "    \n",
    "for train_index , test_index in kf.split(electronics):\n",
    "    data_df = electronics\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"electronics\",f1,acc, mcc])\n",
    "    \n",
    "df_result = pd.DataFrame(data, columns = ['category', 'f1-score', 'accuracy', 'matthews-corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.groupby(df_result.category).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('../results/ensemble-REE.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: tensorflow]",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
