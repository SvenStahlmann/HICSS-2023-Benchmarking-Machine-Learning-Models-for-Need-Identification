{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oDi2Zu6Wduss"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.legacy.data import Iterator, BucketIterator\n",
    "import torchtext.datasets\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/labeled/combined.csv\")\n",
    "electronics = df.groupby(df.category).get_group(\"Electronics\")\n",
    "pet = df.groupby(df.category).get_group(\"Pet supplies\")\n",
    "baby = df.groupby(df.category).get_group(\"Baby\")\n",
    "sports = df.groupby(df.category).get_group(\"Sport outdoors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mLibIFbDUmTu"
   },
   "outputs": [],
   "source": [
    "#%% Prepare the dataset via torchtext\n",
    "spacy_en = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner', 'textcat'\n",
    "                                     'entity_ruler', 'sentencizer', \n",
    "                                     'merge_noun_chunks', 'merge_entities',\n",
    "                                     'merge_subtokens'])\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "  \n",
    "# set up fields\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataformat(dataframe, TEXT, LABEL):\n",
    "    dataframe.to_csv(\".tmp/file.csv\", index=False)\n",
    "    \n",
    "    datafield = [('text', TEXT),  ('label', LABEL)]\n",
    "    dataset = TabularDataset(path ='.tmp/file.csv',  \n",
    "                             format='csv',\n",
    "                             skip_header=True,\n",
    "                             fields=datafield)  \n",
    "    \n",
    "    iterator =  Iterator(\n",
    "        dataset, \n",
    "        batch_size=64,\n",
    "        device=torch.device('cuda'), \n",
    "        sort_within_batch=False,\n",
    "        repeat=False)\n",
    "    \n",
    "    \n",
    "    return iterator, dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SSjRipJEYm5w"
   },
   "outputs": [],
   "source": [
    "#%% Text CNN model\n",
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        #load pretrained embedding in embedding layer.\n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "    \n",
    "        #Convolutional Layers with different window size kernels\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        #Dropout layer\n",
    "        #self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        #FC layer\n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        \n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        \n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "\n",
    "        #fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "        \n",
    "\n",
    "#%% Training the Model\n",
    "def train(model, device, train_itr, optimizer, epoch, max_epoch):\n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0,0\n",
    "    for batch in train_itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text,0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "    \n",
    "    size = len(train_itr.dataset)\n",
    "    train_loss /= size \n",
    "    accuracy = 100.0 * corrects/size\n",
    "  \n",
    "    return train_loss, accuracy\n",
    "    \n",
    "def valid(model, device, test_itr):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    targets = []\n",
    "    corrects, test_loss = 0.0,0\n",
    "    for batch in test_itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text,0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "\n",
    "        targets = targets + target.tolist()\n",
    "        pred = pred + result.tolist()\n",
    "    \n",
    "    size = len(test_itr.dataset)\n",
    "    test_loss /= size \n",
    "    accuracy = 100.0 * corrects/size\n",
    "\n",
    "    return pred, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataframe_train, dataframe_train_test, verbose=False):\n",
    "    # Creating field for text and label\n",
    "    TEXT = Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "    LABEL = Field(sequential=False)\n",
    "    #clean the text\n",
    "    TEXT.preprocessing = torchtext.legacy.data.Pipeline(clean_str)\n",
    "    \n",
    "    train_iter, train_dataset = generate_dataformat(dataframe_train, TEXT, LABEL)\n",
    "    test_iter, test= generate_dataformat(dataframe_train_test, TEXT, LABEL)\n",
    "    \n",
    "    TEXT.build_vocab(train_dataset, vectors= 'glove.6B.300d')\n",
    "    LABEL.build_vocab(train_dataset)\n",
    "\n",
    "    vocab = TEXT.vocab\n",
    "    \n",
    "    model = textCNN(vocab, 300, 100, [3,3,3, 4,4,4 , 5,5,5] , 2).to('cuda')\n",
    "    if(verbose): print(model)\n",
    "    \n",
    "    # Use GPU if it is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #optimizer\n",
    "    f1 = 0;\n",
    "    acc = 0;\n",
    "    mcc = 0;\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(1, 10+1):\n",
    "        #train loss\n",
    "        tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
    "        if(verbose): print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
    "\n",
    "        pred, targets = valid(model, device, test_iter)\n",
    "        f1 = f1_score(targets, pred, average=\"macro\")\n",
    "        acc = accuracy_score(targets, pred)\n",
    "        mcc = matthews_corrcoef(targets,pred)\n",
    "        \n",
    "        if(verbose): print('Valid Epoch: {} \\t f1: {}% \\t acc: {}'.format(epoch, f1, acc))\n",
    "    return f1, acc, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 2)\n",
    "data = []\n",
    "\n",
    "for train_index , test_index in kf.split(baby):\n",
    "    data_df = baby\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"baby\",f1,acc, mcc])\n",
    "    \n",
    "for train_index , test_index in kf.split(pet):\n",
    "    data_df = pet\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"pet\",f1,acc, mcc])\n",
    "\n",
    "for train_index , test_index in kf.split(sports):\n",
    "    data_df = sports\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"sports\",f1,acc, mcc])\n",
    "    \n",
    "for train_index , test_index in kf.split(electronics):\n",
    "    data_df = electronics\n",
    "    train_df = data_df.iloc[train_index]\n",
    "    test_df =  data_df.iloc[test_index]\n",
    "    f1, acc, mcc = evaluation(train_df, test_df)\n",
    "    data.append([\"electronics\",f1,acc, mcc])\n",
    "    \n",
    "df_result = pd.DataFrame(data, columns = ['category', 'f1-score', 'accuracy', 'matthews-corr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1-score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>matthews-corr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>0.785389</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.575294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electronics</th>\n",
       "      <td>0.753665</td>\n",
       "      <td>0.771390</td>\n",
       "      <td>0.517761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet</th>\n",
       "      <td>0.793550</td>\n",
       "      <td>0.826500</td>\n",
       "      <td>0.591958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>0.749745</td>\n",
       "      <td>0.766875</td>\n",
       "      <td>0.501422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1-score  accuracy  matthews-corr\n",
       "category                                      \n",
       "baby         0.785389  0.792000       0.575294\n",
       "electronics  0.753665  0.771390       0.517761\n",
       "pet          0.793550  0.826500       0.591958\n",
       "sports       0.749745  0.766875       0.501422"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.groupby(df_result.category).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('../results/cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "textCNN_IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [pipenv: ml_default]",
   "language": "python",
   "name": "ml_default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
